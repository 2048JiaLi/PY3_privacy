## [动态规划(Dynamic Programming)详解](https://github.com/labuladong/fucking-algorithm/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%B3%BB%E5%88%97/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%AF%A6%E8%A7%A3%E8%BF%9B%E9%98%B6.md)
- [斐波那契数列](#斐波那契数列问题——重叠子问题)
  - [暴力递归](#1、暴力递归)
  - [备忘录的递归解法](#2、带备忘录的递归解法)
  - [dp table解法](#3、dp数组的迭代解法)
  - [细节优化](#进一步优化)

**动态规划问题的一般形式就是求最值**。比如，求**最长**递增子序列，**最小**距离等

**求解动态规划的核心问题是穷举**。因为求最值，也就是把所有可行答案穷举，然后找到最值。

>> 动态规划的穷举不同之处在于此类问题**存在「重叠子问题」**，如果暴力穷举会导致效率极其低下，所以需要「备忘录」或「DP table」来优化穷举过程，避免不必要的计算（重复的计算）

而且，动态规划问题一定会**具备「最优子结构」**，才可以通过子问题的最值得到原问题的最值。

另外，虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出**正确的「状态转移方程」**才能正确地穷举。

- 以上提到的，就是动态规划三要素：重叠子问题、最优子结构、状态转移方程。
> **写出状态转移方程是最困难的**

## 斐波那契数列问题——重叠子问题
> 注：斐波那契数列严格来说不是动态规划问题，这里用于说明什么是**重叠子问题**

### 1、暴力递归

斐波那契数列的数学形式就是递归的，写成代码就是这样：

```py
def fib(N):
    if N == 1 or N == 2:
        return 1
    return fib(N-1) + fib(N-2)
```

假设$n = 20$，则其递归树为

> PS：但凡遇到需要递归的问题，最好都画出递归树，这对你分析算法的复杂度，寻找算法低效的原因都有巨大帮助。

![](./image/递归树.jpg)

**递归算法的时间复杂度 = 子问题个数 × 解决一个子问题需要的时间。**

子问题个数，即递归树中节点的总数。显然二叉树节点总数为指数级别，所以子问题个数为$O(2^{n})$。

解决一个子问题的时间，在本算法中，没有循环，只有 $f(n - 1) + f(n - 2)$一个加法操作，时间为$O(1)$。

所以，这个算法的时间复杂度为 $O(2^n)$，指数级别。

观察递归树，很明显发现了算法低效的原因：**存在大量重复计算**，比如 $f(18)$被计算了两次，而且你可以看到，以 $f(18)$ 为根的这个递归树体量巨大，多算一遍，会耗费巨大的时间。

这就是动态规划问题的**第一个性质：重叠子问题**。下面，我们想办法解决这个问题。

### 2、带备忘录的递归解法

即然耗时的原因是重复计算，那么我们可以造一个「备忘录」，每次算出某个子问题的答案后别急着返回，先记到「备忘录」里再返回；每次遇到一个子问题先去「备忘录」里查一查，如果发现之前已经解决过这个问题了，直接把答案拿出来用，不要再耗时去计算了。

可以使用数组作为「备忘录」，或者哈希表（字典）
```py
from collections import defaultdict

class fib(object):
    def __init__(self):
        self.merry = defaultdict(int)
        self.merry[1] = 1
        self.merry[2] = 1
    
    def fun(self, N):
        if self.merry[N]:
            return self.merry[N]
        
        self.merry[N] = self.fun(N-1) + self.fun(N-2)
        return self.merry[N]
```
> $n=20$时提高了70倍

实际上，带「备忘录」的递归算法，把一棵存在巨量冗余的递归树通过「剪枝」，改造成了一幅不存在冗余的递归图，极大减少了子问题（即递归图中节点）的个数。

![](./image/备忘录解法子问题.jpg)

子问题个数，即图中节点的总数，由于本算法不存在冗余计算，子问题就是 $f(1), f(2), f(3) ... f(20)$，数量和输入规模 $n = 20$ 成正比，所以子问题个数为 $O(n)$。

解决一个子问题的时间为 $O(1)$。

所以，本算法的时间复杂度是 $O(n)$。暴力算法为$O(2^n)$。

> PS：会有$O(n)$的空间开销，进一步[优化](#进一步优化)可使其将为$O(1)$

至此，带备忘录的递归解法的效率已经和迭代的动态规划解法一样了。实际上，这种解法和迭代的动态规划已经差不多了，只不过**这种方法叫做「自顶向下」，动态规划叫做「自底向上」**。

**什么是「自顶向下」？**注意我们刚才画的递归树（或者说图），是从上向下延伸，都是从一个规模较大的原问题比如说 $f(20)$，向下逐渐分解规模，直到 $f(1)$ 和 $f(2)$ 触底，然后逐层返回答案，这就叫「自顶向下」。

**什么是「自底向上」？**反过来，我们直接从最底下，最简单，问题规模最小的 $f(1)$ 和 $f(2)$ 开始往上推，直到推到我们想要的答案 $f(20)$，这就是动态规划的思路，这也是为什么动态规划一般都脱离了递归，而是由循环迭代完成计算。

### 3、dp数组的迭代解法
可以把上述「备忘录」独立出来成为一张表，叫做**DP table**，在这张表上完成「自底向上」的推算！
```py
import numpy as np
def fib(N):
    dp_table = np.zeros(N+1)
    dp_table[1] = dp_table[2] = 1

    for i in range(3,N+1):
        dp_table[i] = dp_table[i-1] + dp_table[i-2]

    return dp_table[N]
```

![](.//image/dp_table.jpg)

实际上，带备忘录的递归解法中的「备忘录」，最终完成后就是这个 DP table，所以说这两种解法其实是差不多的，大部分情况下，效率也基本相同。

这里，引出**「状态转移方程」**这个名词，实际上就是描述问题结构的数学形式：

$$
f(n) = 
\begin{cases}
1 , & \text{$n = 1,2$ }\\
f(n-1) + f(n-2) , & \text{$n > 2$ }
\end{cases}
$$

把 $f(n)$ 想做一个状态$n$，这个状态 $n$ 是由状态 $n - 1$ 和状态 $n - 2$ 相加转移而来，这就叫**状态转移**。

>>> 上面的几种解法中的所有操作，例如 `return f(n - 1) + f(n - 2)`，`dp[i] = dp[i - 1] + dp[i - 2]`，以及对备忘录或 DP table 的初始化操作，都是围绕这个方程式的不同表现形式。可见列出「状态转移方程」的重要性，它是解决问题的核心。很容易发现，其实状态转移方程直接代表着暴力解法。

**动态规划问题最困难的就是写出状态转移方程**，即这个暴力解。优化方法无非是用备忘录或者 DP table，再无奥妙可言。

### 进一步优化
根据斐波那契数列的状态转移方程，当前状态只和之前的两个状态有关，其实并不需要那么长的一个 DP table 来存储所有的状态，只要想办法存储之前的两个状态就行了。所以，可以进一步优化，**把空间复杂度降为 $O(1)$**：
```py
def fib(N):
    if N == 1 or N == 2:
        return 1
    
    prev , curr = 1,1
    for i in range(3,N+1):
        result = prev + curr
        prev , curr = curr , result

    return curr
```

## 凑零钱问题——最优子结构